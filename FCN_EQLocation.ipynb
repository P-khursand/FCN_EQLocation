{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read, UTCDateTime\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#data loader\n",
    "def read_station(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "\n",
    "def read_catalog(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "\n",
    "def folder_name(eq_dataset, index):\n",
    "    if index < 0 or index > len(eq_dataset):\n",
    "        print(f\"the id should be in range of 0 to {len(eq_dataset)}\")\n",
    "        return -1\n",
    "    else:\n",
    "        year = eq_dataset[\"year\"][index]\n",
    "        month = eq_dataset[\"month\"][index]\n",
    "        day = eq_dataset[\"day\"][index]\n",
    "        hour = eq_dataset[\"hour\"][index]\n",
    "        minute = eq_dataset[\"minute\"][index]\n",
    "        second = eq_dataset[\"second\"][index]\n",
    "        return os.path.join(f\"{year:04d}\",f\"{year:04d}{month:02d}\",\n",
    "                            f\"P{year:04d}{month:02d}{day:02d}.{hour:02d}{minute:02d}{second:05.2f}\")\n",
    "\n",
    "\n",
    "def read_seismogram(eq_dataset, index, base_folder):\n",
    "    if index < 0 or index > len(eq_dataset):\n",
    "        print(f\"the id should be in range of 0 to {len(eq_dataset)}\")\n",
    "        return -1\n",
    "    else:\n",
    "        folder = folder_name(eq_dataset, index)\n",
    "        starttime = UTCDateTime(eq_dataset[\"year\"][index],\n",
    "                                eq_dataset[\"month\"][index],\n",
    "                                eq_dataset[\"day\"][index],\n",
    "                                eq_dataset[\"hour\"][index],\n",
    "                                eq_dataset[\"minute\"][index],\n",
    "                                eq_dataset[\"second\"][index])\n",
    "        data_mask = os.path.join(base_folder, folder, \"*.mseed\")\n",
    "        st = read(data_mask, starttime=starttime)\n",
    "        return st , starttime\n",
    "\n",
    "\n",
    "def process_data(stream, start_time, duration, original_sampling):\n",
    "    for trace in stream:\n",
    "        trace.stats.sampling_rate = original_sampling\n",
    "    end_time = start_time + duration\n",
    "    stream.detrend(type='linear')\n",
    "    stream.taper(type='cosine', max_percentage=0.01)\n",
    "    stream.merge(method=0, fill_value=0)\n",
    "    stream.trim(starttime=start_time, endtime=end_time, pad=True, fill_value=0.0)\n",
    "    for trace in stream:\n",
    "        trace.stats.starttime = start_time\n",
    "    return stream\n",
    "\n",
    "\n",
    "def make_image(stream, stations_data, duration, original_sampling):\n",
    "    stations = stations_data[\"Station\"].values\n",
    "    # print(stations)\n",
    "\n",
    "    E = np.zeros((len(stations_data), duration * original_sampling + 1))\n",
    "    N = np.zeros((len(stations_data), duration * original_sampling + 1))\n",
    "    Z = np.zeros((len(stations_data), duration * original_sampling + 1))\n",
    "\n",
    "    for i in range(len(stations)):\n",
    "        tmp_stream = stream.select(station=stations[i])\n",
    "        for tr in tmp_stream:\n",
    "            # print(tr.stats.channel[-1])\n",
    "            if tr.stats.channel[-1] == \"E\" or tr.stats.channel == \"e\":\n",
    "                E[i, :] = tr.data[:]\n",
    "            elif tr.stats.channel[-1] == \"N\" or tr.stats.channel == \"n\":\n",
    "                N[i, :] = tr.data[:]\n",
    "            elif tr.stats.channel[-1] == \"Z\" or tr.stats.channel == \"z\":\n",
    "                Z[i, :] = tr.data[:]\n",
    "    img = np.array([E[:, :-1], N[:, :-1], Z[:, :-1]])\n",
    "    return img\n",
    "\n",
    "\n",
    "def normalize_image(img):\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            max_w = np.max(np.abs(img[i, j, :])) + 1e-7\n",
    "            img[i, j, :] = (img[i, j, :] / max_w + 1) / 2\n",
    "    return img\n",
    "\n",
    "\n",
    "def location_image(region, earthquake_data, index, longitude_delta, latitude_delta, depth_delta):\n",
    "    longitude = earthquake_data[\"longitude\"].values[index]\n",
    "    latitude = earthquake_data[\"latitude\"].values[index]\n",
    "    depth = earthquake_data[\"depth\"].values[index]\n",
    "    horizontal_error = earthquake_data[\"error_loc\"].values[index]*2\n",
    "    # print(horizontal_error)\n",
    "    depth_error = earthquake_data[\"uncertaint\"].values[index]*10\n",
    "    # print(depth_error1)\n",
    "    # horizontal_error = 10\n",
    "    # depth_error = 5\n",
    "    if region[0] <= longitude <= region[1] and region[2] <= latitude <= region[3] and region[4] <= depth <= region[5]:\n",
    "        x_samples = int((region[1] - region[0]) / longitude_delta)\n",
    "        y_samples = int((region[3] - region[2]) / latitude_delta)\n",
    "        z_samples = int((region[5] - region[4]) / depth_delta)\n",
    "\n",
    "        x = np.linspace(region[0], region[1], x_samples)\n",
    "        y = np.linspace(region[2], region[3], y_samples)\n",
    "        z = np.linspace(region[4], region[5], z_samples)\n",
    "\n",
    "        xx, yy, zz = np.meshgrid(x, y, z, indexing=\"ij\")\n",
    "        # print(longitude, latitude, depth, horizontal_error, type(depth_error))\n",
    "        # print(xx.shape, yy.shape, zz.shape)\n",
    "        loca_image = np.exp(-(\n",
    "                (xx - longitude) ** 2 / (horizontal_error / 110) +\n",
    "                (yy - latitude) ** 2 / (horizontal_error / 110) +\n",
    "                (zz - depth) ** 2 / depth_error))\n",
    "        return loca_image\n",
    "    else:\n",
    "        print(f\"The earthquake id = {id} is not in the region\")\n",
    "        return 1\n",
    "\n",
    "#catalog path folder\n",
    "if __name__ == \"__main__\":\n",
    "    station_file = \"D:\\parwiz/Station.csv\"\n",
    "    catalog_file = \"D:\\parwiz/Catalog6.csv\"\n",
    "\n",
    "    base_folder =  \"D:/parwiz/\"\n",
    "\n",
    "    stations = read_station(station_file)\n",
    "    earthquakes = read_catalog(catalog_file)\n",
    "\n",
    "    print(len(earthquakes))\n",
    "    \n",
    "# read earthquake data \n",
    "    st , start_time= read_seismogram(earthquakes, 1, base_folder)\n",
    "    st = process_data(st, start_time, 120,50)\n",
    "    # print(\"--------------------------->\",start_time)\n",
    "    # for tr in st:\n",
    "    #     print(tr.stats.starttime, tr.stats.npts, tr.stats.channel)\n",
    "   \n",
    "#wave form convert to image    \n",
    "    image = make_image(st, stations, 120, 50)\n",
    "    image = normalize_image(image)\n",
    "    print(image.shape,np.max(image), np.min(image))\n",
    "    print(image[1,:,:].shape)\n",
    "    #plt.figure(figsize=[6,8])\n",
    "    plt.imshow(100**image[1,:,:],  aspect='auto', cmap=\"hot\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    loc_image = location_image([45, 47, 33, 36, 0, 30], earthquakes, 1, 0.02, 0.02, 1.0)\n",
    "    print(loc_image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "from obspy import read, UTCDateTime\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#convert image to tiff format\n",
    "def read_station(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "\n",
    "def read_catalog(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "\n",
    "def folder_name(eq_dataset, index):\n",
    "    if index < 0 or index > len(eq_dataset):\n",
    "        print(f\"the id should be in range of 0 to {len(eq_dataset)}\")\n",
    "        return -1\n",
    "    else:\n",
    "        year = eq_dataset[\"year\"][index]\n",
    "        month = eq_dataset[\"month\"][index]\n",
    "        day = eq_dataset[\"day\"][index]\n",
    "        hour = eq_dataset[\"hour\"][index]\n",
    "        minute = eq_dataset[\"minute\"][index]\n",
    "        second = eq_dataset[\"second\"][index]\n",
    "        return os.path.join(f\"{year:04d}\", f\"{year:04d}{month:02d}\",\n",
    "                            f\"P{year:04d}{month:02d}{day:02d}.{hour:02d}{minute:02d}{second:05.2f}\")\n",
    "\n",
    "\n",
    "def read_seismogram(eq_dataset, index, base_folder):\n",
    "    if index < 0 or index > len(eq_dataset):\n",
    "        print(f\"the id should be in range of 0 to {len(eq_dataset)}\")\n",
    "        return -1\n",
    "    else:\n",
    "        folder = folder_name(eq_dataset, index)\n",
    "        starttime = UTCDateTime(eq_dataset[\"year\"][index],\n",
    "                                eq_dataset[\"month\"][index],\n",
    "                                eq_dataset[\"day\"][index],\n",
    "                                eq_dataset[\"hour\"][index],\n",
    "                                eq_dataset[\"minute\"][index],\n",
    "                                eq_dataset[\"second\"][index])\n",
    "        data_mask = os.path.join(base_folder, folder, \"*.mseed\")\n",
    "        st = read(data_mask, starttime=starttime)\n",
    "        return st, starttime\n",
    "\n",
    "\n",
    "def process_data(stream, start_time, duration, original_sampling):\n",
    "    for trace in stream:\n",
    "        trace.stats.sampling_rate = original_sampling\n",
    "    end_time = start_time + duration\n",
    "    stream.detrend(type='linear')\n",
    "    stream.taper(type='cosine', max_percentage=0.01)\n",
    "    stream.merge(method=0, fill_value=0)\n",
    "    stream.trim(starttime=start_time, endtime=end_time, pad=True, fill_value=0.0)\n",
    "    for trace in stream:\n",
    "        trace.stats.starttime = start_time\n",
    "    return stream\n",
    "\n",
    "\n",
    "def make_image(stream, stations_data, duration, original_sampling):\n",
    "    stations = stations_data[\"Station\"].values\n",
    "    # print(stations)\n",
    "\n",
    "    E = np.zeros((len(stations_data), duration * original_sampling + 1))\n",
    "    N = np.zeros((len(stations_data), duration * original_sampling + 1))\n",
    "    Z = np.zeros((len(stations_data), duration * original_sampling + 1))\n",
    "\n",
    "    for i in range(len(stations)):\n",
    "        tmp_stream = stream.select(station=stations[i])\n",
    "        for tr in tmp_stream:\n",
    "            # print(tr.stats.channel[-1])\n",
    "            if tr.stats.channel[-1] == \"E\" or tr.stats.channel == \"e\":\n",
    "                E[i, :] = tr.data[:]\n",
    "            elif tr.stats.channel[-1] == \"N\" or tr.stats.channel == \"n\":\n",
    "                N[i, :] = tr.data[:]\n",
    "            elif tr.stats.channel[-1] == \"Z\" or tr.stats.channel == \"z\":\n",
    "                Z[i, :] = tr.data[:]\n",
    "    img = np.array([E[:, :-1], N[:, :-1], Z[:, :-1]])\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_image(img, filename):\n",
    "    min_image = np.min(image)\n",
    "    max_image = np.max(image)\n",
    "    img = (img - min_image) / ((max_image - min_image)+1e-8)\n",
    "    tifffile.imwrite(filename, img.astype(np.float32))\n",
    "\n",
    "\n",
    "def location_image(region, earthquake_data, index, longitude_delta, latitude_delta, depth_delta, filename):\n",
    "    longitude = earthquake_data[\"longitude\"].values[index]\n",
    "    latitude = earthquake_data[\"latitude\"].values[index]\n",
    "    depth = earthquake_data[\"depth\"].values[index]\n",
    "    horizontal_error = earthquake_data[\"error_loc\"].values[index] * 2\n",
    "    # print(horizontal_error)\n",
    "    depth_error = earthquake_data[\"uncertaint\"].values[index] * 10\n",
    "    # print(depth_error1)\n",
    "    # horizontal_error = 10\n",
    "    # depth_error = 5\n",
    "    if region[0] <= longitude <= region[1] and region[2] <= latitude <= region[3] and region[4] <= depth <= region[5]:\n",
    "        x_samples = int((region[1] - region[0]) / longitude_delta)\n",
    "        y_samples = int((region[3] - region[2]) / latitude_delta)\n",
    "        z_samples = int((region[5] - region[4]) / depth_delta)\n",
    "\n",
    "        x = np.linspace(region[0], region[1], x_samples)\n",
    "        y = np.linspace(region[2], region[3], y_samples)\n",
    "        z = np.linspace(region[4], region[5], z_samples)\n",
    "\n",
    "        xx, yy, zz = np.meshgrid(x, y, z, indexing=\"ij\")\n",
    "        # print(longitude, latitude, depth, horizontal_error, type(depth_error))\n",
    "        # print(xx.shape, yy.shape, zz.shape)\n",
    "        loca_image = np.exp(-(\n",
    "                (xx - longitude) ** 2 / (horizontal_error / 110) +\n",
    "                (yy - latitude) ** 2 / (horizontal_error / 110) +\n",
    "                (zz - depth) ** 2 / depth_error))\n",
    "\n",
    "        loca_image = np.transpose(loca_image, [2, 0, 1])\n",
    "        tifffile.imwrite(filename, loca_image.astype(np.float32))\n",
    "        # print(loca_image.shape)\n",
    "\n",
    "    else:\n",
    "        print(f\"The earthquake id = {id} is not in the region\")\n",
    "        exit(1)\n",
    "#data path folder \n",
    "if __name__ == \"__main__\":\n",
    "    station_file = \"E:/New Data FCN/Station.csv\"\n",
    "    catalog_file = \"E:/New Data FCN/Catalog6.csv\"\n",
    "    base_folder = \"E:/New Data FCN/\"\n",
    "\n",
    "    report_file = open(\"report.txt\", \"w\")\n",
    "\n",
    "    if not os.path.exists(os.path.join(base_folder, \"data\", \"images\")):\n",
    "        print(\"creating image database folder ...\")\n",
    "        os.makedirs(os.path.join(base_folder, \"data\", \"images\"))\n",
    "        \n",
    "# save image path folder\n",
    "    image_folder = os.path.join(base_folder, \"data\", \"images\")\n",
    "    if not os.path.exists(os.path.join(base_folder, \"data\", \"labels\")):\n",
    "        print(\"creating labels database folder ...\")\n",
    "        os.makedirs(os.path.join(base_folder, \"data\", \"labels\"))\n",
    "    label_folder = os.path.join(base_folder, \"data\", \"labels\")\n",
    "\n",
    "    stations = read_station(station_file)\n",
    "    earthquakes = read_catalog(catalog_file)\n",
    "\n",
    "    print(len(earthquakes))\n",
    "    for i in range(4459,len(earthquakes)):\n",
    "        # print(i,\"sarted ...\")\n",
    "        st, start_time = read_seismogram(earthquakes, i, base_folder)\n",
    "        st = process_data(st, start_time, 120, 50)\n",
    "        st_filter = st.filter(type=\"bandpass\", freqmin=1, freqmax=10)\n",
    "        image = make_image(st_filter, stations, 120, 50)\n",
    "        image_name = f\"{image_folder}/f{i:04d}.tiff\"\n",
    "        # print(image.shape)\n",
    "        save_image(image, image_name)\n",
    "        label_name = f\"{label_folder}/l{i:04d}.tiff\"\n",
    "        # print(label_name)\n",
    "        report_file.write(f\"{i}, {image_name}, {np.min(image)}, {np.max(image)}\")\n",
    "        location_image([45, 47, 33, 36, 0, 30], earthquakes, i, 0.02, 0.02, 1.0, label_name)\n",
    "        print(i, \"finished\")\n",
    "\n",
    "    report_file.close()\n",
    "    print(\"bye bye ...\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c89129a7fb5fde0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tifffile\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_correlation_accuracy(x, y, threshold=0.1, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Calculates the correlation coefficient for each tensor in the batch and accuracy.\n",
    "\n",
    "    Args:\n",
    "        x: A tensor of shape (N, C, H, W).\n",
    "        y: A tensor of shape (N, C, H, W).\n",
    "        threshold: Threshold for determining correct estimations (default is 0.1).\n",
    "        eps: Small value to prevent division by zero and handle floating-point precision.\n",
    "\n",
    "    Returns:\n",
    "        correlations: A tensor of shape (N) containing the correlation coefficient for each image in the batch.\n",
    "        accuracy: An integer representing the number of correct estimations based on the threshold.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x_flat = x.view(N, -1)\n",
    "    y_flat = y.view(N, -1)\n",
    "    # Flatten tensors to (N, -1) for simplicity\n",
    "\n",
    "    # Compute means\n",
    "    x_mean = x_flat.mean(dim=1, keepdim=True)\n",
    "    y_mean = y_flat.mean(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute deviations\n",
    "    x_dev = x_flat - x_mean\n",
    "    y_dev = y_flat - y_mean\n",
    "\n",
    "    # Compute variances\n",
    "    x_var = (x_dev ** 2).sum(dim=1)\n",
    "    y_var = (y_dev ** 2).sum(dim=1)\n",
    "\n",
    "    # Compute covariance\n",
    "    covariance = (x_dev * y_dev).sum(dim=1)\n",
    "\n",
    "    # Initialize correlations tensor\n",
    "    correlations = torch.empty(N)\n",
    "\n",
    "    # Identify zero variance cases\n",
    "    zero_var_mask = (x_var.abs() < eps) & (y_var.abs() < eps)\n",
    "    non_zero_var_mask = ~zero_var_mask\n",
    "\n",
    "    # Handle zero variance cases\n",
    "    if zero_var_mask.any():\n",
    "        # Compute scaling factor k\n",
    "        x_mean_nonzero = x_mean[zero_var_mask].abs() > eps\n",
    "        y_mean_nonzero = y_mean[zero_var_mask].abs() > eps\n",
    "        both_nonzero = x_mean_nonzero & y_mean_nonzero\n",
    "        both_zero = ~x_mean_nonzero & ~y_mean_nonzero\n",
    "\n",
    "        # Handle cases where both means are non-zero (proportional relationship)\n",
    "        correlations[zero_var_mask] = torch.where(\n",
    "            both_nonzero.squeeze(),\n",
    "            torch.tensor(1.0),\n",
    "            torch.tensor(float('nan'))\n",
    "        )\n",
    "\n",
    "        # Handle cases where both means are zero (identical constants)\n",
    "        correlations[zero_var_mask] = torch.where(\n",
    "            both_zero.squeeze(),\n",
    "            torch.tensor(1.0),\n",
    "            correlations[zero_var_mask]\n",
    "        )\n",
    "\n",
    "        # For cases where one mean is zero and the other is not, correlation is undefined\n",
    "        correlations[zero_var_mask] = torch.where(\n",
    "            ~both_nonzero.squeeze() & ~both_zero.squeeze(),\n",
    "            torch.tensor(float('nan')),\n",
    "            correlations[zero_var_mask]\n",
    "        )\n",
    "\n",
    "    # Compute correlations for non-zero variance cases\n",
    "    correlations[non_zero_var_mask] = covariance[non_zero_var_mask] / (\n",
    "            torch.sqrt(x_var[non_zero_var_mask] * y_var[non_zero_var_mask]) + eps\n",
    "    )\n",
    "\n",
    "    # Compute accuracy (exclude NaN values)\n",
    "    valid_correlations = correlations[~correlations.isnan()]\n",
    "    accuracy = (valid_correlations >= threshold).sum().item()\n",
    "\n",
    "    return correlations, accuracy\n",
    "\n",
    "\n",
    "# Data Generator\n",
    "def generate_data(base_folder, index):\n",
    "    image_name = f\"f{index:04d}.tiff\"\n",
    "    label_name = f\"l{index:04d}.tiff\"\n",
    "    image_path = os.path.join(base_folder, \"data\", \"images\", image_name)\n",
    "    label_path = os.path.join(base_folder, \"data\", \"labels\", label_name)\n",
    "    image = tifffile.imread(image_path)\n",
    "    # print(f\"image-{image_name} - Shape: {image.shape}\")\n",
    "    label = tifffile.imread(label_path)\n",
    "    # print(f\"label-{label_name} - Shape: {label.shape}\")\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    def __init__(self, base_folder, indices):\n",
    "        self.base_folder = base_folder\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idd = self.indices[index]\n",
    "        x, y = generate_data(self.base_folder, idd)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "# data split to train, validation , test\n",
    "def get_indices(earthquakes, split='train'):\n",
    "    total_len = len(earthquakes)\n",
    "    indices = np.arange(len(earthquakes))\n",
    "    total_len = len(earthquakes)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    if split == \"train\":\n",
    "        return indices[:int(0.7 * total_len)]\n",
    "    elif split == \"validation\":\n",
    "        return indices[int(0.7 * total_len): int(0.9 * total_len)]\n",
    "    elif split == \"test\":\n",
    "        return indices[int(0.9 * total_len): total_len]\n",
    "\n",
    "\n",
    "# Settings and Data Loading\n",
    "from settings import BASE_FOLDR, CATALOG, BATCH_SIZE\n",
    "from read_data import read_catalog\n",
    "\n",
    "catalog_file = CATALOG\n",
    "base_folder = BASE_FOLDR\n",
    "\n",
    "earthquakes = read_catalog(catalog_file)\n",
    "print(len(earthquakes))\n",
    "\n",
    "train_indices = get_indices(earthquakes, \"train\")\n",
    "val_indices = get_indices(earthquakes, \"validation\")\n",
    "test_indices = get_indices(earthquakes, \"test\")\n",
    "\n",
    "train_dataset = EarthquakeDataset(base_folder, train_indices)\n",
    "val_dataset = EarthquakeDataset(base_folder, val_indices)\n",
    "test_dataset = EarthquakeDataset(base_folder, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# kk = 0\n",
    "# for x_val, y_val in train_loader:\n",
    "#     kk += 1\n",
    "#     print(kk, x_val.size(), y_val.size())\n",
    "\n",
    "# Model Definition\n",
    "class FCNloca(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNloca, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(1, 4))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(1, 4))\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 4))\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=(3, 4))\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up6 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=(25, 1), mode='bilinear', align_corners=True),\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=(2, 2), mode='bilinear', align_corners=True),\n",
    "        )\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=(2, 2), mode='bilinear', align_corners=True),\n",
    "        )\n",
    "\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.conv9 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.up9 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=(1, 126), mode='bilinear', align_corners=True),\n",
    "            nn.MaxPool2d(kernel_size=(1, 39))\n",
    "        )\n",
    "\n",
    "        self.conv10 = nn.Sequential(\n",
    "            nn.Conv2d(32, 30,  kernel_size=3, padding=(1,2)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"----->0\",x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(\"----->1\",x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(\"----->2\",x.shape)\n",
    "        x = self.conv3(x)\n",
    "        # print(\"---->3\",x.shape)\n",
    "        x = self.conv4(x)\n",
    "        # print(\"----->4\",x.shape)\n",
    "        x = self.conv5(x)\n",
    "        # print(\"----->5\",x.shape)\n",
    "        x = self.up6(x)\n",
    "        # print(\"----->6_0\",x.shape)\n",
    "        x = self.conv6(x)\n",
    "        # print(\"----->6_1\",x.shape)\n",
    "        x = self.up7(x)\n",
    "        x = self.conv7(x)\n",
    "        # print(\"----->7\",x.shape)\n",
    "        x = self.up8(x)\n",
    "        x = self.conv8(x)\n",
    "        # print(\"----->8\",x.shape)\n",
    "        x = self.conv9(x)\n",
    "        x = self.up9(x)\n",
    "        # print(\"----->9\",x.shape)\n",
    "        x = self.conv10(x)\n",
    "        # print(\"----->10\",x.shape)\n",
    "        return x\n",
    "\n",
    "def train_model(train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = FCNloca().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "    if not os.path.exists(\"model\"):\n",
    "        os.mkdir(\"model\")\n",
    "    PATH = os.path.join(os.getcwd(), \"model\", \"model\")\n",
    "\n",
    "    num_epochs = 50\n",
    "    acc_train = 0\n",
    "    acc_val = 0\n",
    "\n",
    "    fid = open(\"output.txt\", \"w\")\n",
    "\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_n = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            batch_n += 1\n",
    "            # print(\"---------------------------->\",x_batch.shape, y_batch.shape)\n",
    "            print(f'{epoch = }, {batch_n = }')\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(x_batch)\n",
    "            # print(\"----------------->\",outputs)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_batch_cpu = y_batch.to(\"cpu\")\n",
    "            outputs_cpu = outputs.to(\"cpu\")\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, tmp_acc = calculate_correlation_accuracy(y_batch_cpu, outputs_cpu, threshold=0.67, eps=1e-8)\n",
    "            acc_train += tmp_acc\n",
    "\n",
    "        fid.write(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}, Accuracy: {acc_train / len(train_loader)} \")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "        # Save model at predefined Path\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }, PATH)\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "\n",
    "                outputs = model(x_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "                # scheduler.step(val_loss)\n",
    "                y_val_cpu = y_val.to(\"cpu\")\n",
    "                outputs_cpu = outputs.to(\"cpu\")\n",
    "                _, tmp_acc = calculate_correlation_accuracy(y_val_cpu, outputs_cpu, threshold=0.67, eps=1e-8)\n",
    "                acc_val += tmp_acc\n",
    "\n",
    "        fid.write(\n",
    "            f\", Validation Loss: {val_loss / len(val_loader)}, Validation Accuracy: {acc_val / len(val_loader)}\\n\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "    fid.close()\n",
    "\n",
    "    # After all epochs, perform a test loop\n",
    "    # After all epochs, perform a test loop\n",
    "    fid = open(\"test_output.txt\", \"w\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "            outputs = model(x_test)\n",
    "            loss = criterion(outputs, y_test)\n",
    "            test_loss += loss.item()\n",
    "            y_test_cpu = y_test.to(\"cpu\")\n",
    "            outputs_cpu = outputs.to(\"cpu\")\n",
    "\n",
    "            _, tmp_acc = calculate_correlation_accuracy(y_test_cpu, outputs_cpu, threshold=0.67, eps=1e-8)\n",
    "            acc_test += tmp_acc\n",
    "\n",
    "    fid.write(f\"Test Loss: {test_loss / len(test_loader)}, Test Validation: {acc_test / len(test_loader)}\\n\")\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader)}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model(train_loader, val_loader, test_loader)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7c923201626a40c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from settings import  REGION, LATITUDE_DELTA, LONGITUDE_DELTA, DEPTH_DELTA\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from read_data import read_catalog\n",
    "\n",
    "base_folder = \"E:/FCN_FOLDER/پایان/RESULTS_FCN\"\n",
    "# earthquakes = read_catalog(catalog_file)\n",
    "# print(len(earthquakes))\n",
    "# image path folder\n",
    "labels = glob.glob(os.path.join(base_folder,\"l*.tiff\"))\n",
    "outputs = glob.glob(os.path.join(base_folder,\"o*.tiff\"))\n",
    "#region coordinate\n",
    "region = REGION\n",
    "longitude_delta = LONGITUDE_DELTA\n",
    "latitude_delta = LATITUDE_DELTA\n",
    "depth_delta = DEPTH_DELTA\n",
    "\n",
    "x_samples = int((region[1] - region[0]) / longitude_delta)\n",
    "y_samples = int((region[3] - region[2]) / latitude_delta)\n",
    "z_samples = int((region[5] - region[4]) / depth_delta)\n",
    "\n",
    "x = np.linspace(region[0], region[1], x_samples)\n",
    "y = np.linspace(region[2], region[3], y_samples)\n",
    "z = np.linspace(region[4], region[5], z_samples)\n",
    "\n",
    "delta_x = x[1] - x[0]\n",
    "delta_y = y[1] - y[0]\n",
    "delta_z = z[1] - z[0]\n",
    "h_error = []\n",
    "z_error =[]\n",
    "fid = open(\"results.txt\", \"w\")\n",
    "#extract loop for earthquake location\n",
    "for i in range(len(labels)):\n",
    "    label = tifffile.imread(labels[i])\n",
    "    name = labels[i].split(\"\\\\\")[-1]\n",
    "    output = tifffile.imread(outputs[i])\n",
    "    zl, lonl, latl = np.unravel_index(label.argmax(), label.shape)\n",
    "    zo, lono, lato = np.unravel_index(output.argmax(), output.shape)\n",
    "    h_err = np.sqrt(((lonl-lono)*delta_x)**2+((latl-lato)*delta_y)**2)*110\n",
    "    z_err = np.abs(zl-zo)*delta_z\n",
    "    h_error.append(h_err)\n",
    "    z_error.append(z_err)\n",
    "    fid.write(f\"{name} {i} {lonl} {latl} {zl} {lono} {lato} {zo} {region[0]+lonl*delta_x} {region[2]+latl*delta_y} \"\n",
    "              f\"{zl*delta_z} {region[0]+lono*delta_x} {region[2]+lato*delta_y} {zo*delta_z} {h_err} {z_err}\\n\")\n",
    "\n",
    "print(np.mean(h_error), np.std(h_error))\n",
    "print(np.mean(z_error), np.std(z_error))\n",
    "\n",
    "fid.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4711de685e8a144"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
